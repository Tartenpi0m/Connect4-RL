{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5971313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f9c984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pettingzoo\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a00dbb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1d72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import random as rd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69e510",
   "metadata": {},
   "source": [
    "# Using the PettingZoo environment\n",
    "\n",
    "This notebook provides smalls chunks of code to get you started with the Connect4 project. You do not have to use this code in you final file, but you can if you wish to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc87362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7, 2)\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# The main difference with the standard gym api is the way the environment is queried. The `step` method return `None`. To get the data on the environment, use the `last` method\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "# state is a dictionary with two keys: observation and action_mask\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ")  # Observation is a numpy array with three coordinates, indicating the positions of the pieces of of player 0 and 1 on the the board\n",
    "print(state[\"observation\"][:, :, 0])  # Where the pieces of player 0 are\n",
    "print(state[\"observation\"][:, :, 1])  # Where the pieces of player 1 are\n",
    "\n",
    "print(state[\"action_mask\"])  # an array showing whether the actions are legal or nots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbd836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "print(\n",
    "    state[\"observation\"].shape\n",
    ") \n",
    "print(state[\"observation\"][:, :, 0])  \n",
    "print(state[\"observation\"][:, :, 1])  \n",
    "\n",
    "print(state[\"action_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5bcdf",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here are some implementations of trivial agents that you should be able to beat ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f7e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            self.rng = np.random.default_rng()\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self.name = \"Random Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        return self.random_choice_with_mask(np.arange(7), obs_mask[\"action_mask\"])\n",
    "\n",
    "    def random_choice_with_mask(self, arr, mask):\n",
    "        masked_arr = np.ma.masked_array(arr, mask=1 - mask)\n",
    "        if masked_arr.count() == 0:\n",
    "            return None\n",
    "        return self.rng.choice(masked_arr.compressed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayLeftmostLegal:\n",
    "    def __init__(self):\n",
    "        self.name = \"Left Player\"\n",
    "\n",
    "    def get_action(self, obs_mask, epsilon=None):\n",
    "        for i, legal in enumerate(obs_mask[\"action_mask\"]):\n",
    "            if legal:\n",
    "                return i\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fdf7e",
   "metadata": {},
   "source": [
    "# Running a game\n",
    "\n",
    "\n",
    "The following function runs a full game between the two agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent0, agent1, display=False):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    while not done:\n",
    "        for i, agent in enumerate([agent0, agent1]):\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            env.step(action)\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            done = terminated\n",
    "            if np.sum(obs[\"action_mask\"]) == 0:\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                return 0.5\n",
    "            if done:\n",
    "                if display:\n",
    "                    print(f\"Player {i}: {agent.name} won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1dd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = RandomPlayer()\n",
    "agent1 = PlayLeftmostLegal()\n",
    "\n",
    "play_game(env, agent0, agent1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4544abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([play_game(env, agent0, agent1, display=False) for _ in range(100)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cc5af",
   "metadata": {},
   "source": [
    "# Emulating a Gym environment\n",
    "\n",
    "If we fix the opposite policy, the game from the point of view of the agent is equivalent to a Gym environment. The following class implements this simulation. Then any algorithm that would work in a gym environment with the same observations will work here. \n",
    "\n",
    "Note that we implemented the possibility to be the first or the second player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a2406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvAgainstPolicy: \n",
    "    def __init__(self, env, policy, first_player=True):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.first_player = first_player\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward, terminated, _, _ = self.env.last()\n",
    "        if terminated: \n",
    "            self.last_step = obs, reward, True, False, {}\n",
    "        else: \n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "            obs, reward, terminated, _, _ = self.env.last()\n",
    "            self.last_step = obs, -reward, terminated, False, {}\n",
    "        return self.last_step\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        if not(self.first_player): \n",
    "            obs, _, _, _, _ = self.env.last()\n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "\n",
    "        self.last_step = self.env.last()\n",
    "        return self.last_step\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3fab1",
   "metadata": {},
   "source": [
    "# Evaluating an agent against a fixed policy: \n",
    "\n",
    "Using the environment above, we can evaluate the agent against this fixed policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_policy(env, agent, policy, N_episodes=10, first_player=True):\n",
    "    eval_env = EnvAgainstPolicy(env, policy, first_player=first_player)\n",
    "    results = []\n",
    "    for _ in range(N_episodes):\n",
    "        done = False\n",
    "        eval_env.reset()\n",
    "        obs, _, _, _, _ = eval_env.last()\n",
    "        while not done:\n",
    "            action = agent.get_action(obs, epsilon=0)\n",
    "            eval_env.step(action)\n",
    "            obs, reward, done, _, _ = eval_env.last()\n",
    "        results.append(reward)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1790b4",
   "metadata": {},
   "source": [
    "We can see that if both players play randomly, there is a small but significant advantage to the first player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38debdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=False))\n",
    "plt.show()\n",
    "plt.hist(eval_against_policy(env, RandomPlayer(), RandomPlayer(), N_episodes=1000, first_player=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79580594",
   "metadata": {},
   "source": [
    "# Your turn \n",
    "\n",
    "Try to build a decent agent. Be creative! You can try any idea that you have: the grade is not about performance of the agent, but more about illustrating phenomena happening in Reinforcement Learning for turn-based games. It's okay to 'help' the agent in any way, as long as it follows the ideas of RL (i.e., as long as there is some learning involved).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9da4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38c5dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tuple(a):\n",
    "    try:\n",
    "        return tuple(numpy_to_tuple(i) for i in a)\n",
    "    except TypeError:\n",
    "        return a\n",
    "    \n",
    "class MCController: \n",
    "    \"\"\"\n",
    "        Monte-Carlo control\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 observation_space,\n",
    "                 gamma=0.99, \n",
    "                 eps_init=.5, \n",
    "                 eps_min=1e-5,\n",
    "                 eps_step=1e-3,\n",
    "                 episodes_between_greedyfication=500,\n",
    "                 name='MC Controller'\n",
    "                ):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.name = name\n",
    "\n",
    "        self.epsilon = eps_init\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_step = eps_step\n",
    "\n",
    "        self.episodes_between_greedyfication = episodes_between_greedyfication\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def eps_greedy(self, obs, action_mask, epsilon=None):\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        if np.random.random() < self.epsilon: \n",
    "            temp_mask = self.action_space[action_mask]\n",
    "            return np.random.choice(temp_mask)\n",
    "        if len(action_mask) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            actions = self.q_values[obs]\n",
    "            print(np.where(action_mask, actions , np.inf))\n",
    "            temp_mask = np.where(actions, action_mask, np.inf)\n",
    "            return np.random.choice(np.flatnonzero(temp_mask == np.max(temp_mask)))\n",
    "\n",
    "        \n",
    "    def get_action(self, state, epsilon=None): \n",
    "        return self.eps_greedy(numpy_to_tuple(state[\"observation\"]), state[\"action_mask\"])\n",
    "        \n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        self.current_episode_rewards.append(reward)\n",
    "        self.current_episode_obs.append((numpy_to_tuple(state[\"observation\"]), action))\n",
    "        \n",
    "        if terminated:\n",
    "            self._end_of_episode_update()\n",
    "\n",
    "    def _end_of_episode_update(self):\n",
    "        current_episode_returns = rewardseq_to_returns(self.current_episode_rewards, self.gamma)\n",
    "        seen = []\n",
    "        for i, (state, action) in enumerate(self.current_episode_obs): \n",
    "            if (state, action) not in seen: \n",
    "                seen.append((state, action))\n",
    "                return_value = current_episode_returns[i]\n",
    "                \n",
    "                n = self.number_of_values_in_estimate[state, action]\n",
    "                self.values_estimates[state][action] = n / (n + 1) * self.values_estimates[state][action] + 1 / (n+1) * return_value\n",
    "                self.number_of_values_in_estimate[state, action] += 1\n",
    "                            \n",
    "        self.current_episode_rewards = []\n",
    "        self.current_episode_obs = []\n",
    "\n",
    "        self.episode_counter += 1 \n",
    "        \n",
    "        if self.episode_counter % self.episodes_between_greedyfication == 0:\n",
    "            new_q_values = defaultdict(lambda: np.zeros(self.action_space))\n",
    "            for state in self.values_estimates: \n",
    "                new_q_values[state] = self.values_estimates[state]\n",
    "            self.reset(q_values=new_q_values)\n",
    "            self.epsilon_decay()\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        self.eps = max(self.eps - self.eps_step, self.eps_min)\n",
    "            \n",
    "    def reset(self, q_values=None):\n",
    "        self.current_episode_rewards = []\n",
    "        self.current_episode_obs = []\n",
    "\n",
    "        if q_values is None:\n",
    "            self.q_values = defaultdict(lambda: np.zeros(self.action_space))\n",
    "        else: \n",
    "            self.q_values = q_values\n",
    "\n",
    "        self.values_estimates = defaultdict(lambda: np.zeros(self.action_space))\n",
    "        self.number_of_values_in_estimate = defaultdict(lambda : 0)\n",
    "        \n",
    "gamma = .999\n",
    "N_episodes = 10_000\n",
    "\n",
    "episodes_between_greedyfication = 500\n",
    "\n",
    "eps_init = .1\n",
    "eps_step= eps_init * episodes_between_greedyfication / N_episodes\n",
    "\n",
    "action_space = list(range(7))\n",
    "observation_space = (list(range(6)), list(range(7)), list(range(2)))\n",
    "\n",
    "mc_agent = MCController(action_space, observation_space, eps_init=eps_init, eps_step=eps_step, gamma=gamma, episodes_between_greedyfication=episodes_between_greedyfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84219c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_2(env, agent1, agent2, N_episodes=10, first_player=True, display=False):\n",
    "    eval_env = EnvAgainstPolicy(env, policy, first_player=first_player)\n",
    "\n",
    "    for i in tqdm.tqdm(range(N_episodes)):\n",
    "        if display and (i+1) % agent1.episodes_between_greedyfication == 0:\n",
    "            print(i+1, \"out of\", N_episodes)\n",
    "\n",
    "        agent = agent1 if eval_env.next_player == 1 else agent2\n",
    "        done = False\n",
    "        eval_env.reset()\n",
    "        state, reward, terminated, truncated, _ = eval_env.last()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            eval_env.step(action)\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.last()\n",
    "            agent.update(state, action, reward, terminated, next_state)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "    return eval_env, agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3d34b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .999\n",
    "N_episodes = 10_000\n",
    "\n",
    "episodes_between_greedyfication = 500\n",
    "\n",
    "eps_init = .1\n",
    "eps_step= eps_init * episodes_between_greedyfication / N_episodes\n",
    "\n",
    "action_space = list(range(7))\n",
    "observtation_space = (list(range(6)), list(range(7)), list(range(2)))\n",
    "\n",
    "mc_agent = MCController(action_space, observation_space, eps_init=eps_init, eps_step=eps_step, gamma=gamma, episodes_between_greedyfication=episodes_between_greedyfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c99a9c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7,) (0,1,2,3,4,5,6) () ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRandomPlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m(env, agent0, agent1, display)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([agent0, agent1]):\n\u001b[1;32m----> 7\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m display:\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mMCController.get_action\u001b[1;34m(self, state, epsilon)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_to_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 46\u001b[0m, in \u001b[0;36mMCController.eps_greedy\u001b[1;34m(self, obs, action_mask, epsilon)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values[obs]\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m     temp_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(actions, action_mask, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39mflatnonzero(temp_mask \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(temp_mask)))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7,) (0,1,2,3,4,5,6) () "
     ]
    }
   ],
   "source": [
    "play_game(env, mc_agent, RandomPlayer(), display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d1f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
